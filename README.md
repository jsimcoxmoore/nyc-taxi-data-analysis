
# NYC Taxi Data Ingestion and Processing

This README provides detailed instructions on how to ingest, process, and serve NYC Taxi trip data using Tinybird. The goal is to filter trips with distances above the 0.9 percentile and serve the results via a real-time API. You will also learn how to interact with Tinybird’s auto-generated Python code and manage edge cases, scalability, and assumptions.

## Prerequisites

- A Tinybird account (sign up at [Tinybird](https://tinybird.co)).
- A valid Tinybird API Token.
- Basic knowledge of SQL.
- Access to a Python environment (local or cloud-based like Google Colab) to run the Python script generated by Tinybird.

## Steps to Set Up and Run the Script

### Step 1: Log into Tinybird

1. Open your browser and navigate to [Tinybird’s login page](https://tinybird.co).
2. Log into your Tinybird account or create a new one.

### Step 2: Create a New Data Source (Single File or Multiple Files)

1. From the Tinybird dashboard, click on **Data Sources** in the left-hand menu.
2. Select **New Data Source** and choose **Remote URL** as the source type.
3. Paste the URL of the Parquet file from the NYC Taxi dataset that you want to process. For example:
   
   ```
   https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet
   ```

4. Name the data source descriptively, such as `nyc_taxi_january_2024`.
5. Click **Ingest Data** to import the Parquet file into Tinybird.
6. Once the file is ingested, Tinybird will display a preview of the data. Verify that all columns, including `trip_distance`, are correctly ingested.

### Option: Loop Through and Process Multiple Files

If you want to process more than one file (e.g., an entire year’s worth of taxi data), you can use a Python script to loop through all the Parquet files available on the NYC Taxi website.

The NYC Taxi dataset provides monthly Parquet files with URLs following this structure:
```
https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_<year>-<month>.parquet
```

You can process these files either sequentially or in parallel.

#### Example 1: Sequential Processing (Basic Loop)

```python
import requests

# Define the base URL
base_url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_'

# Define the range of years and months
years = [2023, 2024]
months = range(1, 13)

# Loop through each year and month, ingesting each file
for year in years:
    for month in months:
        file_url = f'{base_url}{year}-{month:02}.parquet'
        response = requests.post(
            f'https://api.us-east.aws.tinybird.co/v0/datasources?url={file_url}',
            headers={'Authorization': 'Bearer YOUR_TINYBIRD_TOKEN'}
        )
        if response.status_code == 200:
            print(f'Successfully ingested {file_url}')
        else:
            print(f'Failed to ingest {file_url}: {response.status_code}')
```

#### Example 2: Parallel Processing for Speed (Using `concurrent.futures`)

```python
import requests
from concurrent.futures import ThreadPoolExecutor

def ingest_file(file_url, token):
    response = requests.post(
        f'https://api.us-east.aws.tinybird.co/v0/datasources?url={file_url}',
        headers={'Authorization': f'Bearer {token}'}
    )
    if response.status_code == 200:
        print(f'Successfully ingested {file_url}')
    else:
        print(f'Failed to ingest {file_url}: {response.status_code}')

# Example usage with parallel processing
years = [2023, 2024]
months = range(1, 13)
token = 'YOUR_TINYBIRD_TOKEN'

# Construct all the file URLs in advance
file_urls = [f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-{month:02}.parquet'
             for year in years for month in months]

# Use ThreadPoolExecutor to run requests in parallel
with ThreadPoolExecutor() as executor:
    executor.map(lambda url: ingest_file(url, token), file_urls)
```

### Step 3: Create a New Pipe

1. After the data is ingested, go to the **Pipes** section in the left-hand menu.
2. Click **New Pipe** and select the data source `nyc_taxi_january_2024`.
3. In the Pipe editor, write the SQL query to filter trips above the 0.9 percentile based on `trip_distance`.

   Example SQL Query:
   ```sql
   SELECT *
   FROM nyc_taxi_january_2024
   WHERE trip_distance > (
       SELECT quantileExact(0.9)(trip_distance)
       FROM nyc_taxi_january_2024
   )
   ```

4. Click **Preview** to ensure the query returns the expected results.

### Step 4: Create an Endpoint for the Query

1. In the Pipe editor, click **+ New Node** and select **Endpoint**.
2. Name the endpoint, such as `filtered_trips_api`.
3. Confirm that the data being exposed by this endpoint matches your filtered query.
4. Click **Preview** to ensure the endpoint serves the correct data.

### Step 5: Publish the Endpoint

1. Once the endpoint is set up and verified, click **Publish** at the top right corner.
2. Tinybird will generate an API endpoint URL. Example:
   ```
   https://api.us-east.tinybird.co/v0/pipes/filtered_trips_api.json?token=YOUR_TINYBIRD_TOKEN
   ```
3. Copy the API URL for testing.

### Step 6: Test the API

1. Open your browser and paste the API URL to retrieve the filtered dataset in JSON format.
2. Alternatively, use `cURL` in your terminal:
   ```bash
   curl "https://api.us-east.tinybird.co/v0/pipes/filtered_trips_api.json?token=YOUR_TINYBIRD_TOKEN"
   ```
3. Verify the JSON response contains only the trips where `trip_distance` exceeds the 0.9 percentile.

### Step 7: Using the Auto-Generated Python Code from Tinybird

1. Copy the auto-generated Python code provided by Tinybird.
   
   Example Python Code:
   ```python
   import requests

   params = {
       'token': 'YOUR_TINYBIRD_TOKEN'
   }

   url = f'https://api.us-east.aws.tinybird.co/v0/pipes/filtered_trips_api.csv'
   response = requests.get(url, params=params)
   print(response.text)
   ```

2. Paste the code into your Python environment and replace `'YOUR_TINYBIRD_TOKEN'` with your actual token.
3. Run the script to print the data in CSV format.

### Optional: Save the Data as CSV

To save the response as a CSV file, modify the script slightly:

```python
import requests

params = {
    'token': 'YOUR_TINYBIRD_TOKEN'
}

url = f'https://api.us-east.aws.tinybird.co/v0/pipes/filtered_trips_api.csv'
response = requests.get(url, params=params)

# Save the CSV response
with open('filtered_trips.csv', 'w') as file:
    file.write(response.text)
```

## Summary

- **Sequential Processing**: Easier to implement but slower for large datasets.
- **Parallel Processing**: Faster and ideal for large datasets.

Choose the method that fits your needs: simplicity with sequential processing or speed with parallel processing.
