
# NYC Taxi Data Ingestion and Processing

This README provides detailed instructions on how to ingest, process, and serve NYC Taxi trip data using Tinybird. The goal is to filter trips with distances above the 0.9 percentile and serve the results via a real-time API. You will also learn how to interact with Tinybird’s auto-generated Python code and manage edge cases, scalability, and assumptions.

## Prerequisites

- A Tinybird account (sign up at [Tinybird](https://tinybird.co)).
- A valid Tinybird API Token.
- Basic knowledge of SQL.
- Access to a Python environment (local or cloud-based like Google Colab) to run the Python script generated by Tinybird.

## Steps to Set Up and Run the Script

### Step 1: Log into Tinybird

1. Open your browser and navigate to [Tinybird’s login page](https://tinybird.co).
2. Log into your Tinybird account or create a new one.

### Step 2: Create a New Data Source (Single File or Multiple Files)

1. From the Tinybird dashboard, click on **Data Sources** in the left-hand menu.
2. Select **New Data Source** and choose **Remote URL** as the source type.
3. Paste the URL of the Parquet file from the NYC Taxi dataset that you want to process. For example:
   
   ```
   https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet
   ```

4. Name the data source descriptively, such as `nyc_taxi_january_2024`.
5. Click **Ingest Data** to import the Parquet file into Tinybird.
6. Once the file is ingested, Tinybird will display a preview of the data. Verify that all columns, including `trip_distance`, are correctly ingested.


### Option: Loop Through and Process Multiple Files

If you want to process more than one file (e.g., an entire year’s worth of taxi data), you can use a Python script to loop through all the Parquet files available on the NYC Taxi website.

The NYC Taxi dataset provides monthly Parquet files with URLs following this structure:
```
https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_<year>-<month>.parquet
```

You can process these files either sequentially or in parallel.

#### Example 1: Sequential Processing (Basic Loop)

```python
import requests

# Define the base URL
base_url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_'

# Define the range of years and months
years = [2023, 2024]
months = range(1, 13)

# Loop through each year and month, ingesting each file
for year in years:
    for month in months:
        file_url = f'{base_url}{year}-{month:02}.parquet'
        response = requests.post(
            f'https://api.us-east.aws.tinybird.co/v0/datasources?url={file_url}',
            headers={'Authorization': 'Bearer YOUR_TINYBIRD_TOKEN'}
        )
        if response.status_code == 200:
            print(f'Successfully ingested {file_url}')
        else:
            print(f'Failed to ingest {file_url}: {response.status_code}')
```

#### Example 2: Parallel Processing for Speed (Using `concurrent.futures`)

```python
import requests
from concurrent.futures import ThreadPoolExecutor

def ingest_file(file_url, token):
    response = requests.post(
        f'https://api.us-east.aws.tinybird.co/v0/datasources?url={file_url}',
        headers={'Authorization': f'Bearer {token}'}
    )
    if response.status_code == 200:
        print(f'Successfully ingested {file_url}')
    else:
        print(f'Failed to ingest {file_url}: {response.status_code}')

# Example usage with parallel processing
years = [2023, 2024]
months = range(1, 13)
token = 'YOUR_TINYBIRD_TOKEN'

# Construct all the file URLs in advance
file_urls = [f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-{month:02}.parquet'
             for year in years for month in months]

# Use ThreadPoolExecutor to run requests in parallel
with ThreadPoolExecutor() as executor:
    executor.map(lambda url: ingest_file(url, token), file_urls)
```

### Step 3: Create a New Pipe

1. After the data is ingested, go to the **Pipes** section in the left-hand menu.
2. Click **New Pipe** and select the data source `nyc_taxi_january_2024`.
3. In the Pipe editor, write the SQL query to filter trips above the 0.9 percentile based on `trip_distance`.

   Example SQL Query:
   ```sql
   SELECT *
   FROM nyc_taxi_january_2024
   WHERE trip_distance > (
       SELECT quantileExact(0.9)(trip_distance)
       FROM nyc_taxi_january_2024
   )
   ```

4. Click **Preview** to ensure the query returns the expected results.

### Step 4: Create an Endpoint for the Query

1. In the Pipe editor, click **+ New Node** and select **Endpoint**.
2. Name the endpoint, such as `filtered_trips_api`.
3. Confirm that the data being exposed by this endpoint matches your filtered query.
4. Click **Preview** to ensure the endpoint serves the correct data.

### Step 5: Publish the Endpoint

1. Once the endpoint is set up and verified, click **Publish** at the top right corner.
2. Tinybird will generate an API endpoint URL. Example:
   ```
   https://api.us-east.tinybird.co/v0/pipes/filtered_trips_api.json?token=YOUR_TINYBIRD_TOKEN
   ```
3. Copy the API URL for testing.

### Step 6: Test the API

1. Open your browser and paste the API URL to retrieve the filtered dataset in JSON format.
2. Alternatively, use `cURL` in your terminal:
   ```bash
   curl "https://api.us-east.tinybird.co/v0/pipes/filtered_trips_api.json?token=YOUR_TINYBIRD_TOKEN"
   ```
3. Verify the JSON response contains only the trips where `trip_distance` exceeds the 0.9 percentile.

### Step 7: Using the Auto-Generated Python Code from Tinybird

1. Copy the auto-generated Python code provided by Tinybird.
   
   Example Python Code:
   ```python
   import requests

   params = {
       'token': 'YOUR_TINYBIRD_TOKEN'
   }

   url = f'https://api.us-east.aws.tinybird.co/v0/pipes/filtered_trips_api.csv'
   response = requests.get(url, params=params)
   print(response.text)
   ```

2. Paste the code into your Python environment and replace `'YOUR_TINYBIRD_TOKEN'` with your actual token.
3. Run the script to print the data in CSV format.

### Optional: Save the Data as CSV

To save the response as a CSV file, modify the script slightly:

```python
import requests

params = {
    'token': 'YOUR_TINYBIRD_TOKEN'
}

url = f'https://api.us-east.aws.tinybird.co/v0/pipes/filtered_trips_api.csv'
response = requests.get(url, params=params)

# Save the CSV response
with open('filtered_trips.csv', 'w') as file:
    file.write(response.text)
```

## Assumptions

During the implementation of this project, the following questions and assumptions were made to clarify the scope and approach to data processing and ingestion:

1. **Dataset Scope**:  
   - It was initially unclear if all available Parquet files from the NYC Yellow Taxi dataset should be processed, or if a specific subset (such as certain time periods or trip types) was preferred. Clarification was sought to ensure the correct dataset was analyzed.
   
2. **Distance Calculation**:  
   - The 0.9 percentile calculation is based on the `trip_distance` column. Clarification was requested on whether additional filtering (such as outlier removal or handling of extreme values) should be performed before calculating the percentile, or if the distance should be calculated exactly as provided in the raw dataset without adjustments.
   - Further clarification was sought on whether any data cleaning steps were required, such as handling invalid or extreme `trip_distance` values.

3. **Code and Output Format**:  
   - It was assumed that CSV is a sufficient format for output unless specified otherwise (such as JSON or Parquet). Confirmation was requested on whether there was a preferred structure or format for the output data.
   - Additional clarification was also requested on the inclusion of specific sections in the README, such as error handling, edge case considerations, and scalability approaches.

## Edge Cases

When working with the NYC Yellow Taxi dataset, the following edge cases were identified and considered:

1. **Invalid or Extreme Trip Distances**:  
   - Some trips may have incorrect or extreme values for `trip_distance`, such as negative distances or extremely high values that may represent data entry errors. To address this, basic data cleaning will involve removing or capping extreme values based on a reasonable threshold (such as trips with a distance above a certain value like 100 miles, which are outliers).
   
2. **Missing or Corrupt Data**:  
   - While unlikely, it is possible that some records in the Parquet files could have missing or corrupt values in critical columns such as `trip_distance`. A check should be implemented to ignore or flag these rows during the ingestion process.
   
3. **Date Range Filtering**:  
   - There could be instances where only certain date ranges are relevant for analysis, or where specific holidays or events skew the data. Filtering by date can help to reduce the influence of such anomalies in the dataset.
   
4. **Handling Incomplete Files**:  
   - If a file is incomplete or only partially ingested due to network or data integrity issues, this should be logged, and retries or manual interventions may be required.

## Scalability Considerations

Although the solution focuses on processing a single Parquet file for the assessment, the approach to scaling the solution for multiple or all files from the NYC Yellow Taxi dataset was considered:

1. **Batch Processing**:  
   - If all the monthly Parquet files need to be processed, the system should be designed to handle batch ingestion. Each file can be ingested and processed in sequence or parallel (depending on system resources and time constraints), with options for incremental loading to ensure data integrity.
   
2. **Parallel Processing**:  
   - To speed up processing, parallelization can be applied using Python’s `concurrent.futures` or other multi-threading libraries. By processing files concurrently, the total processing time can be reduced significantly, particularly for large datasets.
   
3. **Data Partitioning**:  
   - Partitioning the data by date or other dimensions (such as taxi zones) can help improve query performance when analyzing large datasets. This allows for faster filtering and querying by minimizing the amount of data processed in each query.
   
4. **Scaling with Distributed Systems**:  
   - For large-scale production use cases, where the full dataset needs to be processed and queried in real time, a distributed processing platform such as Apache Spark or AWS EMR could be used. These platforms are built to handle large datasets across multiple machines, providing both scalability and fault tolerance.
   
5. **Trade-offs**:  
   - Sequential processing is simple to implement but slower, particularly for large datasets. Parallel processing offers faster results but requires more setup and resource management. Depending on the scale of the dataset and system constraints, a balance must be struck between simplicity and performance.
